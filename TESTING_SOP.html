<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Testing SOP + RAG Architecture Commentary</title>
<style>
  :root {
    --bg: #0d1117;
    --surface: #161b22;
    --surface2: #1c2333;
    --border: #30363d;
    --text: #e6edf3;
    --text-muted: #8b949e;
    --accent: #58a6ff;
    --accent-green: #3fb950;
    --accent-yellow: #d29922;
    --accent-red: #f85149;
    --accent-purple: #bc8cff;
    --code-bg: #0d1117;
  }

  * { box-sizing: border-box; margin: 0; padding: 0; }

  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.7;
    padding: 2rem;
    max-width: 960px;
    margin: 0 auto;
  }

  h1 {
    font-size: 2rem;
    font-weight: 700;
    margin-bottom: 0.5rem;
    color: var(--text);
    border-bottom: 2px solid var(--accent);
    padding-bottom: 0.75rem;
  }

  h2 {
    font-size: 1.5rem;
    font-weight: 600;
    margin-top: 2.5rem;
    margin-bottom: 1rem;
    color: var(--accent);
    border-bottom: 1px solid var(--border);
    padding-bottom: 0.5rem;
  }

  h3 {
    font-size: 1.2rem;
    font-weight: 600;
    margin-top: 2rem;
    margin-bottom: 0.75rem;
    color: var(--accent-purple);
  }

  h4 {
    font-size: 1.05rem;
    font-weight: 600;
    margin-top: 1.5rem;
    margin-bottom: 0.5rem;
    color: var(--accent-green);
  }

  p { margin-bottom: 1rem; }

  hr {
    border: none;
    border-top: 1px solid var(--border);
    margin: 2.5rem 0;
  }

  a { color: var(--accent); text-decoration: none; }
  a:hover { text-decoration: underline; }

  strong { color: #fff; }

  code {
    font-family: 'JetBrains Mono', 'Fira Code', 'Cascadia Code', monospace;
    font-size: 0.88em;
    background: var(--surface2);
    padding: 0.15em 0.4em;
    border-radius: 4px;
    color: var(--accent);
  }

  pre {
    background: var(--code-bg);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 1rem 1.25rem;
    overflow-x: auto;
    margin-bottom: 1.25rem;
    font-size: 0.9em;
    line-height: 1.5;
  }

  pre code {
    background: none;
    padding: 0;
    color: var(--accent-green);
  }

  table {
    width: 100%;
    border-collapse: collapse;
    margin-bottom: 1.5rem;
    font-size: 0.92em;
  }

  th {
    background: var(--surface2);
    color: var(--accent);
    font-weight: 600;
    text-align: left;
    padding: 0.6rem 0.8rem;
    border: 1px solid var(--border);
  }

  td {
    padding: 0.5rem 0.8rem;
    border: 1px solid var(--border);
    vertical-align: top;
  }

  tr:nth-child(even) td {
    background: var(--surface);
  }

  ul, ol {
    margin-bottom: 1rem;
    padding-left: 1.5rem;
  }

  li { margin-bottom: 0.4rem; }

  /* Checkbox styling */
  .checklist {
    list-style: none;
    padding-left: 0;
  }

  .checklist li {
    position: relative;
    padding-left: 2rem;
    margin-bottom: 0.5rem;
  }

  .checklist li::before {
    content: '';
    position: absolute;
    left: 0;
    top: 0.35em;
    width: 16px;
    height: 16px;
    border: 2px solid var(--border);
    border-radius: 3px;
    background: var(--surface);
  }

  /* Info/callout boxes */
  .callout {
    background: var(--surface);
    border-left: 4px solid var(--accent);
    border-radius: 0 8px 8px 0;
    padding: 1rem 1.25rem;
    margin-bottom: 1.5rem;
  }

  .callout.green { border-left-color: var(--accent-green); }
  .callout.yellow { border-left-color: var(--accent-yellow); }
  .callout.red { border-left-color: var(--accent-red); }
  .callout.purple { border-left-color: var(--accent-purple); }

  .callout-title {
    font-weight: 700;
    margin-bottom: 0.5rem;
    font-size: 0.95rem;
  }

  .callout.green .callout-title { color: var(--accent-green); }
  .callout.yellow .callout-title { color: var(--accent-yellow); }
  .callout.red .callout-title { color: var(--accent-red); }
  .callout.purple .callout-title { color: var(--accent-purple); }

  /* Day badges */
  .day-badge {
    display: inline-block;
    background: var(--accent);
    color: var(--bg);
    font-weight: 700;
    font-size: 0.8rem;
    padding: 0.2em 0.6em;
    border-radius: 4px;
    margin-right: 0.5rem;
    vertical-align: middle;
  }

  .tag {
    display: inline-block;
    font-size: 0.75rem;
    font-weight: 600;
    padding: 0.15em 0.5em;
    border-radius: 12px;
    margin-left: 0.3rem;
  }

  .tag-auto {
    background: rgba(63, 185, 80, 0.15);
    color: var(--accent-green);
    border: 1px solid rgba(63, 185, 80, 0.3);
  }

  .tag-manual {
    background: rgba(210, 153, 34, 0.15);
    color: var(--accent-yellow);
    border: 1px solid rgba(210, 153, 34, 0.3);
  }

  .tag-cli {
    background: rgba(88, 166, 255, 0.15);
    color: var(--accent);
    border: 1px solid rgba(88, 166, 255, 0.3);
  }

  /* Results table placeholder */
  .placeholder {
    color: var(--text-muted);
    font-style: italic;
  }

  /* Navigation sidebar / TOC */
  .toc {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 1.25rem;
    margin-bottom: 2rem;
  }

  .toc-title {
    font-weight: 700;
    color: var(--accent);
    margin-bottom: 0.75rem;
    font-size: 1rem;
  }

  .toc ul {
    list-style: none;
    padding-left: 0;
  }

  .toc li {
    margin-bottom: 0.3rem;
  }

  .toc a {
    color: var(--text-muted);
    font-size: 0.92em;
  }

  .toc a:hover {
    color: var(--accent);
  }

  .toc .indent {
    padding-left: 1.25rem;
  }
</style>
</head>
<body>

<h1>Testing SOP + RAG Architecture Commentary</h1>

<!-- Paper Title & Description -->
<div class="callout green" style="margin-top:1.5rem;">
  <div class="callout-title">Paper Title</div>
  <p style="font-size:1.1rem; font-weight:600; margin-bottom:0.75rem;">RAG4Report: Multi-Query Decomposition and Structured Evaluation for Long-Form Technical Report Generation</p>
  <div class="callout-title" style="margin-top:0.75rem;">System Description</div>
  <p style="margin-bottom:0;">A two-stage retrieval-augmented generation system that decomposes report-level prompts into sub-questions, retrieves and deduplicates evidence across multiple embedding-space regions, and synthesizes structured, citation-grounded technical reports.</p>
</div>

<!-- Table of Contents -->
<div class="toc">
  <div class="toc-title">Contents</div>
  <ul>
    <li><a href="#part-a">Part A: What's Already Implemented (Code Complete)</a></li>
    <li><a href="#part-b">Part B: Your RAG Type &mdash; What It Is and Why It Matters</a></li>
    <li><a href="#part-c">Part C: Day-by-Day Testing Checklist</a>
      <ul class="indent">
        <li><a href="#prereqs">Prerequisites</a></li>
        <li><a href="#day1">Day 1: Smoke Test</a></li>
        <li><a href="#day2">Day 2: Full Evaluation (10 Prompts)</a></li>
        <li><a href="#day3">Day 3: Baseline Comparison + Metrics</a></li>
        <li><a href="#day4">Day 4: Iterate on Weak Spots</a></li>
        <li><a href="#day5">Day 5: Final Run + Paper-Ready Numbers</a></li>
        <li><a href="#day6">Day 6: Buffer + Write-Up</a></li>
      </ul>
    </li>
    <li><a href="#part-d">Part D: Output File Map</a></li>
    <li><a href="#part-e">Part E: Verification Milestones</a></li>
  </ul>
</div>

<hr>

<!-- ═══════════════════════════════════════════════════════════════════ -->
<h2 id="part-a">Part A: What's Already Implemented (Code Complete)</h2>

<div class="callout green">
  <div class="callout-title">All code is written and ready. Nothing needs to be coded &mdash; you just need to run and evaluate.</div>
</div>

<h3>Pipeline Files <span class="tag tag-auto">automatic</span></h3>

<table>
  <thead>
    <tr><th>File</th><th>What it does</th><th>How you use it</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><code>ask.py</code></td>
      <td>CLI entry point. Single-query mode OR report mode (<code>--report</code>).</td>
      <td><code>python ask.py --parser marker --report --q "..." --verbose --save</code></td>
    </tr>
    <tr>
      <td><code>src/rag/report_pipeline.py</code></td>
      <td>Report engine: decomposes prompt &rarr; retrieves per sub-question &rarr; deduplicates &rarr; generates report.</td>
      <td>Called internally by <code>ask.py --report</code>. You don't run this directly.</td>
    </tr>
    <tr>
      <td><code>src/rag/pipeline_e2e.py</code></td>
      <td>Single-query engine: retrieve k=15 &rarr; rerank top-5 &rarr; generate answer.</td>
      <td>Called internally by <code>ask.py</code> (without <code>--report</code>). You don't run this directly.</td>
    </tr>
    <tr>
      <td><code>src/llm/load_generator_llm.py</code></td>
      <td>Loads Gemini 2.0 Flash. <code>max_tokens=2048</code> for single-query, <code>8192</code> for report.</td>
      <td>Automatic based on <code>--report</code> flag.</td>
    </tr>
  </tbody>
</table>

<h3>Evaluation Files <span class="tag tag-cli">you run these</span></h3>

<table>
  <thead>
    <tr><th>File</th><th>What it does</th><th>How you use it</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><code>src/eval/evaluate.py</code></td>
      <td>Runs all 10 golden set prompts through report pipeline, judges each. With <code>--baseline</code>, also runs single-query for comparison.</td>
      <td><code>python -m src.eval.evaluate --parser marker --baseline</code></td>
    </tr>
    <tr>
      <td><code>src/eval/judge.py</code></td>
      <td><code>judge_answer()</code> &mdash; 4 dims for single-query.<br><code>judge_report()</code> &mdash; 6 dims for reports.</td>
      <td>Called automatically by <code>evaluate.py</code>.</td>
    </tr>
    <tr>
      <td><code>src/eval/retrieval_metrics.py</code></td>
      <td>Computes Recall@k, MRR, NDCG by comparing retrieved pages against <code>relevant_pages</code>.</td>
      <td>Called automatically by <code>evaluate.py</code>.</td>
    </tr>
    <tr>
      <td><code>src/eval/golden_set.json</code></td>
      <td>10 report prompts (R01-R10) with <code>expected_sections</code>, <code>must_include_facts</code>, and <code>relevant_pages</code> (currently empty).</td>
      <td><strong>You edit this manually on Day 2.</strong></td>
    </tr>
    <tr>
      <td><code>postprocess.py</code></td>
      <td>Aggregates evaluation JSON into summary stats (means, medians, coverage rates, IR metrics).</td>
      <td><code>python postprocess.py --eval-results &lt;file&gt; --output &lt;file&gt;</code></td>
    </tr>
  </tbody>
</table>

<h3>What <code>--baseline</code> Does</h3>

<div class="callout purple">
  <div class="callout-title">This is your comparison mechanism &mdash; no prior experiments needed</div>
  <p style="margin-bottom:0.5rem;">When you run <code>python -m src.eval.evaluate --parser marker --baseline</code>, for <strong>each</strong> of the 10 prompts:</p>
  <ol>
    <li><strong>Report pipeline:</strong> Decompose &rarr; multi-retrieve (k=15 per sub-question) &rarr; rerank (top-5 per sub-question) &rarr; deduplicate &rarr; merge (~20 chunks) &rarr; generate report (8192 tokens) &rarr; judge with <code>judge_report()</code> (6 dimensions)</li>
    <li><strong>Baseline (single-query):</strong> Same prompt as-is &rarr; retrieve k=15 &rarr; rerank top-5 &rarr; generate answer (2048 tokens) &rarr; judge with <code>judge_answer()</code> (4 dimensions)</li>
  </ol>
  <p style="margin-bottom:0;">The output JSON has <strong>both</strong> side-by-side for each prompt. This is self-contained &mdash; you do NOT need Run 4 or any prior data.</p>
</div>

<h3>CLI Flags Reference</h3>

<pre><code>python ask.py --parser marker --report --q "..."   # Basic report generation
                              --k 20               # Chunks retrieved per sub-question (default: 15)
                              --top-n 7            # Kept after reranking per sub-question (default: 5)
                              --max-chunks 25      # Max unique chunks after dedup (default: 20)
                              --verbose            # Show sub-questions, retrieval metadata, chunk details
                              --save               # Save JSON to query_outputs/</code></pre>

<hr>

<!-- ═══════════════════════════════════════════════════════════════════ -->
<h2 id="part-b">Part B: Your RAG Type &mdash; What It Is and Why It Matters</h2>

<h3>Classification</h3>

<p>Your system is a <strong>Two-Stage Retrieval-Augmented Generation (RAG) pipeline with Query Decomposition for Report Synthesis</strong>. In the RAG taxonomy literature (Gao et al. 2024), this falls under <strong>Advanced RAG</strong>:</p>

<table>
  <thead>
    <tr><th>Component</th><th>Your Choice</th><th>Category</th></tr>
  </thead>
  <tbody>
    <tr><td>Retrieval</td><td>Dense (MiniLM-L12-v2) + Cross-Encoder Reranking (BGE)</td><td>Two-stage retrieval</td></tr>
    <tr><td>Chunking</td><td>Markdown-header-aware + token-budget</td><td>Hierarchical chunking</td></tr>
    <tr><td>Generation</td><td>Gemini 2.0 Flash with citation enforcement</td><td>Constrained generation</td></tr>
    <tr><td>Report extension</td><td>Query decomposition &rarr; multi-retrieval &rarr; synthesis</td><td>Compositional RAG</td></tr>
    <tr><td>Evaluation</td><td>LLM-as-a-judge (6 dims) + IR metrics</td><td>Hybrid evaluation</td></tr>
  </tbody>
</table>

<h3>What Makes This Publishable</h3>

<p><strong>Strengths for a workshop paper:</strong></p>

<ol>
  <li><strong>Domain-specific RAG with math preservation</strong> &mdash; OpenFOAM docs contain LaTeX equations, code blocks, and nested dictionary syntax. Parser comparison (Marker vs Docling vs PyMuPDF) with <code>redo_inline_math=True</code> is a concrete contribution. Most RAG papers test on Wikipedia or generic QA.</li>
  <li><strong>Two-stage retrieval is well-evidenced</strong> &mdash; Dense retrieval for recall (k=15) + cross-encoder reranking for precision (top-5) is current best practice (Nogueira et al., "Passage Re-ranking with BERT").</li>
  <li><strong>Query decomposition for report generation</strong> &mdash; The novel angle for RAG4Report. Single-query RAG retrieves from one region of embedding space and can't cover multiple topics. Decomposing into 3-5 sub-questions solves this. The <code>--baseline</code> comparison makes it empirically testable.</li>
  <li><strong>Checklist-based evaluation</strong> &mdash; <code>expected_sections</code> + <code>must_include_facts</code> with programmatic weighted scoring is more rigorous than pure LLM-as-a-judge.</li>
</ol>

<p><strong>Limitations to acknowledge:</strong></p>

<ol>
  <li><strong>Small embedding model</strong> &mdash; MiniLM-L12-v2 (384-dim). Deliberate trade-off (CPU, fast). Larger models could improve retrieval.</li>
  <li><strong>Single-document corpus</strong> &mdash; One large PDF. Frame as "focused domain evaluation."</li>
  <li><strong>LLM-as-a-judge reliability</strong> &mdash; Gemini 2.5 Flash at temp=0.0. Programmatic <code>overall_score</code> mitigates bias.</li>
  <li><strong>No human evaluation</strong> &mdash; Manually scoring 3-4 reports would strengthen the paper.</li>
</ol>

<div class="callout">
  <div class="callout-title">Paper Narrative</div>
  <p style="margin-bottom:0;">"Single-query RAG fails for report-level tasks; query decomposition with deduplication solves this; here's how to evaluate it with checklist-based judging."</p>
</div>

<hr>

<!-- ═══════════════════════════════════════════════════════════════════ -->
<h2 id="part-c">Part C: Day-by-Day Testing Checklist</h2>

<h3 id="prereqs">Prerequisites</h3>

<div class="callout yellow">
  <div class="callout-title">Before Day 1, ensure:</div>
  <ul class="checklist">
    <li>Vector DB exists: <code>db/marker_db/</code>. If not: <code>python main.py --parser marker --embed</code></li>
    <li><code>GOOGLE_API_KEY</code> is set in your environment</li>
    <li>Dependencies installed: <code>pip install -r requirements.txt</code></li>
  </ul>
</div>

<hr>

<!-- Day 1 -->
<h3 id="day1"><span class="day-badge">DAY 1</span> Smoke Test &mdash; Does the Report Pipeline Work?</h3>

<p><strong>Goal:</strong> Verify end-to-end on a single prompt.</p>

<h4>Step 1.1: Generate one report (default settings)</h4>
<pre><code>python ask.py --parser marker --report --q "Write a technical overview of the discretization framework in OpenFOAM, covering spatial and temporal schemes, the fvSchemes dictionary structure, and how users configure gradient, divergence, Laplacian, interpolation, and surface normal gradient schemes." --verbose --save</code></pre>

<p><strong>Check in terminal:</strong></p>
<ul class="checklist">
  <li>3-5 sub-questions printed</li>
  <li>Retrieval metadata: <code>total_before_dedup</code>, <code>total_after_dedup</code>, <code>final_chunks_used</code></li>
  <li>Report has <code>##</code> section headings</li>
  <li>Report has <code>[n]</code> inline citations</li>
  <li>Report ends with a <code>## References</code> section</li>
</ul>

<p><strong>Check in saved JSON</strong> (<code>query_outputs/query_TIMESTAMP.json</code>):</p>
<ul class="checklist">
  <li><code>"mode": "report"</code> present</li>
  <li><code>"sub_questions"</code> array has 3-5 entries</li>
  <li><code>"chunks"</code> array has entries with both <code>rerank_score</code> and <code>similarity_score</code></li>
  <li><code>"retrieval_metadata"</code> has the three expected fields</li>
</ul>

<h4>Step 1.2: Try higher k values</h4>
<pre><code>python ask.py --parser marker --report --q "Write a technical overview of the discretization framework in OpenFOAM, covering spatial and temporal schemes, the fvSchemes dictionary structure, and how users configure gradient, divergence, Laplacian, interpolation, and surface normal gradient schemes." --k 20 --top-n 7 --max-chunks 25 --verbose --save</code></pre>

<p><strong>Compare with 1.1:</strong></p>
<ul class="checklist">
  <li><code>final_chunks_used</code> increased?</li>
  <li><code>total_after_dedup</code> &lt; <code>total_before_dedup</code>? (dedup working)</li>
  <li>Report noticeably more detailed?</li>
</ul>

<h4>Step 1.3: Regression check &mdash; single-query still works</h4>
<pre><code>python ask.py --parser marker --q "Explain fvSchemes" --verbose</code></pre>
<ul class="checklist">
  <li>Normal answer generated (not a report, no sub-questions)</li>
</ul>

<div class="callout yellow">
  <div class="callout-title">Decision Point</div>
  <p style="margin-bottom:0;">Based on 1.1 vs 1.2, pick your k values for the rest. Default (k=15, top-n=5, max-chunks=20) is fine unless you see clear improvement with higher values.</p>
</div>

<hr>

<!-- Day 2 -->
<h3 id="day2"><span class="day-badge">DAY 2</span> Full Evaluation &mdash; All 10 Prompts</h3>

<p><strong>Goal:</strong> Generate + judge all 10 golden set reports, then populate ground truth pages.</p>

<h4>Step 2.1: Run evaluation (report pipeline only) <span class="tag tag-cli">CLI</span></h4>
<pre><code>python -m src.eval.evaluate --parser marker</code></pre>

<p><strong>Output:</strong> <code>eval_results/marker_report_evaluation.json</code></p>

<ul class="checklist">
  <li>File contains 10 entries (R01-R10)</li>
  <li>Each entry has <code>"report"</code> &rarr; <code>"evaluation"</code> with 6 scores</li>
  <li>Each entry has <code>"sub_questions"</code> (3-5 per prompt)</li>
  <li>No <code>"error"</code> keys in any evaluation</li>
</ul>

<h4>Step 2.2: Scan report quality <span class="tag tag-manual">MANUAL</span></h4>

<p>Open <code>eval_results/marker_report_evaluation.json</code>. For each R01-R10:</p>
<ul class="checklist">
  <li>Report not truncated (ends properly with References)</li>
  <li>Has multiple <code>##</code> sections (not a wall of text)</li>
  <li>Has <code>[n]</code> citations throughout</li>
  <li>Reasonable length (~1500-2500 words)</li>
</ul>

<p><strong>Note down:</strong> best/worst reports, any obviously wrong judge scores.</p>

<h4>Step 2.3: Populate <code>relevant_pages</code> in golden_set.json <span class="tag tag-manual">MANUAL ~1-2hrs</span></h4>

<p>For each R01-R10:</p>
<ol>
  <li>Look at <code>"chunks"</code> &rarr; <code>"page"</code> values in the evaluation JSON</li>
  <li>Open the OpenFOAM PDF, verify those pages are actually relevant</li>
  <li>Check if the retriever missed any obviously relevant pages</li>
  <li>Update <code>src/eval/golden_set.json</code> &mdash; fill the <code>"relevant_pages": []</code> arrays</li>
</ol>

<div class="callout">
  <div class="callout-title">Why this matters</div>
  <p style="margin-bottom:0;">Without <code>relevant_pages</code>, the IR metrics (Recall@k, MRR, NDCG) can't be computed. Once you fill these in, Day 3's evaluation will automatically include IR numbers.</p>
</div>

<hr>

<!-- Day 3 -->
<h3 id="day3"><span class="day-badge">DAY 3</span> Baseline Comparison + Full Metrics</h3>

<p><strong>Goal:</strong> Get side-by-side numbers (report vs single-query) and aggregate everything.</p>

<h4>Step 3.1: Run evaluation WITH baseline <span class="tag tag-cli">CLI</span></h4>
<pre><code>python -m src.eval.evaluate --parser marker --baseline</code></pre>

<p>This overwrites the previous file. Each entry now has both <code>"report"</code> and <code>"baseline"</code>.</p>

<ul class="checklist">
  <li>Each entry has both <code>"report"</code> and <code>"baseline"</code> keys</li>
  <li>Report entries: 6-dimension scores (groundedness, accuracy, citation, coverage, factual_recall, structure)</li>
  <li>Baseline entries: 4-dimension scores (groundedness, accuracy, citation, completeness)</li>
  <li><code>retrieval_metrics</code> populated for entries with <code>relevant_pages</code></li>
</ul>

<h4>Step 3.2: Aggregate all metrics <span class="tag tag-cli">CLI</span></h4>
<pre><code>python postprocess.py --eval-results eval_results/marker_report_evaluation.json --output eval_results/report_stats.json</code></pre>

<ul class="checklist">
  <li>Overall score mean/median</li>
  <li>Per-dimension means</li>
  <li>Section coverage: X/Y (rate)</li>
  <li>Fact coverage: X/Y (rate)</li>
  <li>IR metrics (if <code>relevant_pages</code> populated)</li>
</ul>

<h4>Step 3.3: Record key findings <span class="tag tag-manual">MANUAL</span></h4>

<table>
  <thead>
    <tr><th>Metric</th><th>Report Pipeline</th><th>Baseline (single-query)</th></tr>
  </thead>
  <tbody>
    <tr><td>Overall Score</td><td class="placeholder">?</td><td class="placeholder">?</td></tr>
    <tr><td>Groundedness</td><td class="placeholder">?</td><td class="placeholder">?</td></tr>
    <tr><td>Technical Accuracy</td><td class="placeholder">?</td><td class="placeholder">?</td></tr>
    <tr><td>Citation Correctness</td><td class="placeholder">?</td><td class="placeholder">?</td></tr>
    <tr><td>Coverage</td><td class="placeholder">?</td><td>N/A</td></tr>
    <tr><td>Factual Recall</td><td class="placeholder">?</td><td>N/A</td></tr>
    <tr><td>Section coverage (found/expected)</td><td class="placeholder">?/?</td><td>N/A</td></tr>
    <tr><td>Fact coverage (found/expected)</td><td class="placeholder">?/?</td><td>N/A</td></tr>
    <tr><td>Chunks used per query</td><td>~20</td><td>5</td></tr>
  </tbody>
</table>

<hr>

<!-- Day 4 -->
<h3 id="day4"><span class="day-badge">DAY 4</span> Iterate on Weak Spots</h3>

<p><strong>Goal:</strong> Diagnose and improve the weakest prompts.</p>

<h4>Step 4.1: Find bottom 3 prompts by <code>overall_score</code> <span class="tag tag-manual">MANUAL</span></h4>

<p>Open <code>eval_results/marker_report_evaluation.json</code> and sort by <code>report.evaluation.overall_score</code>.</p>

<p><strong>Common failure modes:</strong></p>
<table>
  <thead>
    <tr><th>Symptom</th><th>Likely Cause</th><th>Fix</th></tr>
  </thead>
  <tbody>
    <tr><td>Low coverage</td><td>Retriever missed expected sections</td><td>Try <code>--k 20</code></td></tr>
    <tr><td>Low factual recall</td><td>Key facts filtered out during reranking</td><td>Check if facts appear in top-15 but not top-5</td></tr>
    <tr><td>Low citation correctness</td><td>Generator hallucinating citation numbers</td><td>Check chunk content for ambiguity</td></tr>
    <tr><td>Low structure</td><td>Wall of text output</td><td>May need prompt tuning in <code>report_pipeline.py</code></td></tr>
  </tbody>
</table>

<h4>Step 4.2: Re-run weak prompts individually <span class="tag tag-cli">CLI</span></h4>
<pre><code>python ask.py --parser marker --report --q "&lt;weak prompt text&gt;" --k 20 --top-n 7 --max-chunks 25 --verbose --save</code></pre>

<p><strong>Examine:</strong> Are the right chunks being retrieved? Is the generator ignoring relevant chunks?</p>

<h4>Step 4.3: Adjust golden_set.json if needed <span class="tag tag-manual">MANUAL</span></h4>

<p>If some <code>must_include_facts</code> are too specific or not actually in the PDF, revise them. The checklist should only test facts that ARE in the documentation.</p>

<hr>

<!-- Day 5 -->
<h3 id="day5"><span class="day-badge">DAY 5</span> Final Run + Paper-Ready Numbers</h3>

<p><strong>Goal:</strong> Clean final results.</p>

<h4>Step 5.1: Final evaluation run <span class="tag tag-cli">CLI</span></h4>
<pre><code>python -m src.eval.evaluate --parser marker --baseline</code></pre>

<h4>Step 5.2: Final aggregation <span class="tag tag-cli">CLI</span></h4>
<pre><code>python postprocess.py --eval-results eval_results/marker_report_evaluation.json --output eval_results/final_report_stats.json</code></pre>

<h4>Step 5.3: Manual spot-check <span class="tag tag-manual">RECOMMENDED</span></h4>

<p>Pick 2-3 reports. Read them yourself and score on the 6 dimensions (1-5 each). Compare with LLM judge scores. Note agreement/disagreement. This gives you a "human evaluation" data point for the paper.</p>

<hr>

<!-- Day 6 -->
<h3 id="day6"><span class="day-badge">DAY 6</span> Buffer + Write-Up</h3>

<p>Reserved for:</p>
<ul>
  <li>Finalizing the paper/presentation</li>
  <li>Re-running any failed evaluations</li>
  <li>Creating figures/tables from the JSON stats</li>
</ul>

<hr>

<!-- ═══════════════════════════════════════════════════════════════════ -->
<h2 id="part-d">Part D: Output File Map</h2>

<table>
  <thead>
    <tr><th>What</th><th>Path</th><th>When Created</th></tr>
  </thead>
  <tbody>
    <tr><td>Single report (smoke test)</td><td><code>query_outputs/query_YYYYMMDD_HHMMSS.json</code></td><td>Day 1</td></tr>
    <tr><td>Full evaluation (10 prompts + baseline)</td><td><code>eval_results/marker_report_evaluation.json</code></td><td>Day 2 / Day 3</td></tr>
    <tr><td>Aggregated stats</td><td><code>eval_results/report_stats.json</code></td><td>Day 3</td></tr>
    <tr><td>Final stats</td><td><code>eval_results/final_report_stats.json</code></td><td>Day 5</td></tr>
    <tr><td>Golden set (your checklists)</td><td><code>src/eval/golden_set.json</code></td><td>Exists, updated Day 2</td></tr>
  </tbody>
</table>

<hr>

<!-- ═══════════════════════════════════════════════════════════════════ -->
<h2 id="part-e">Part E: Verification Milestones</h2>

<div class="callout green">
  <div class="callout-title">After Day 1</div>
  <p>Report pipeline works. At least 2 saved JSONs in <code>query_outputs/</code>. You've confirmed sub-questions, dedup, citations, and sections all appear.</p>
</div>

<div class="callout">
  <div class="callout-title">After Day 3</div>
  <p>10 reports + 10 baselines judged. Aggregated metrics in hand. You can already draft the results section. Key comparison: report pipeline has coverage/factual_recall scores; baseline doesn't measure these (only has completeness).</p>
</div>

<div class="callout purple">
  <div class="callout-title">After Day 5</div>
  <p style="margin-bottom:0;">Clean final numbers. Optional human evaluation data. Ready to write the paper.</p>
</div>

</body>
</html>
